# 并发与异步编程

## GIL

### GIL 是什么

GIL（Global Interpreter Lock，全局解释器锁）是 CPython 解释器中的一个互斥锁，确保同一时刻只有一个线程执行 Python 字节码。

- 它是 CPython 的实现细节，不是 Python 语言规范的一部分
- 存在的原因是 CPython 的内存管理（引用计数）不是线程安全的
- Jython、PyPy-STM 等实现没有 GIL

```python
import sys

# 查看引用计数，GIL 保护了这类操作的原子性
a = []
print(sys.getrefcount(a))  # 输出引用计数
```

### GIL 对多线程的影响

- CPU 密集型任务：多线程无法真正并行，性能可能比单线程更差（线程切换开销）
- IO 密集型任务：线程在等待 IO 时会释放 GIL，其他线程可以运行，多线程有效

```python
import threading
import time

# GIL 导致 CPU 密集型多线程无法并行
def cpu_task(n):
    count = 0
    while count < n:
        count += 1

start = time.time()

# 单线程执行
cpu_task(50_000_000)
cpu_task(50_000_000)
print(f"单线程耗时: {time.time() - start:.2f}s")

start = time.time()

# 多线程执行（受 GIL 限制，不会更快）
t1 = threading.Thread(target=cpu_task, args=(50_000_000,))
t2 = threading.Thread(target=cpu_task, args=(50_000_000,))
t1.start(); t2.start()
t1.join(); t2.join()
print(f"多线程耗时: {time.time() - start:.2f}s")  # 不会比单线程快
```

### CPU 密集型 vs IO 密集型任务的选择

| 任务类型 | 推荐方案 | 原因 |
|---|---|---|
| CPU 密集型 | 多进程 | 绕过 GIL，真正并行 |
| IO 密集型 | 多线程 / 协程 | IO 等待时释放 GIL |
| 高并发网络 | asyncio 协程 | 单线程，开销最小 |
| 混合型 | 进程 + 协程 | 各取所长 |

```python
# 判断任务类型的简单原则
# CPU 密集型：大量计算、图像处理、数据压缩、加密解密
# IO 密集型：网络请求、文件读写、数据库查询
```

## 多线程

### threading.Thread 的创建与启动

```python
import threading
import time

# 方式一：传入目标函数
def worker(name, delay):
    print(f"线程 {name} 开始")
    time.sleep(delay)
    print(f"线程 {name} 结束")

t = threading.Thread(
    target=worker,
    args=("A",),
    kwargs={"delay": 1},
    name="WorkerThread"  # 可选，给线程命名
)
t.start()   # 启动线程
t.join()    # 等待线程结束

# 方式二：继承 Thread 类，重写 run 方法
class MyThread(threading.Thread):
    def __init__(self, name):
        super().__init__()
        self.name = name

    def run(self):
        # run() 是线程执行的入口
        print(f"自定义线程 {self.name} 运行中")
        time.sleep(0.5)

t2 = MyThread("B")
t2.start()
t2.join()

# 获取当前线程信息
print(threading.current_thread().name)  # 当前线程名
print(threading.active_count())         # 活跃线程数
print(threading.enumerate())            # 所有活跃线程列表
```

### 线程同步（Lock、RLock、Semaphore）

```python
import threading

# --- Lock：互斥锁，防止竞态条件 ---
counter = 0
lock = threading.Lock()

def increment():
    global counter
    for _ in range(100_000):
        with lock:          # 推荐使用 with 语句，自动释放
            counter += 1    # 临界区，同一时刻只有一个线程执行

threads = [threading.Thread(target=increment) for _ in range(5)]
for t in threads: t.start()
for t in threads: t.join()
print(f"counter = {counter}")  # 应为 500000

# --- RLock：可重入锁，同一线程可多次获取 ---
rlock = threading.RLock()

def recursive_task(n):
    with rlock:             # 同一线程可以再次获取，不会死锁
        if n > 0:
            recursive_task(n - 1)

recursive_task(5)

# --- Semaphore：信号量，限制并发数量 ---
semaphore = threading.Semaphore(3)  # 最多允许 3 个线程同时进入

def limited_task(i):
    with semaphore:
        print(f"任务 {i} 执行中（最多 3 个并发）")
        import time; time.sleep(1)

threads = [threading.Thread(target=limited_task, args=(i,)) for i in range(8)]
for t in threads: t.start()
for t in threads: t.join()
```

### threading.Event 与 threading.Condition

```python
import threading
import time

# --- Event：线程间的简单信号通知 ---
event = threading.Event()

def waiter():
    print("等待事件触发...")
    event.wait()            # 阻塞，直到 event 被 set()
    print("事件已触发，继续执行")

def setter():
    time.sleep(2)
    print("触发事件")
    event.set()             # 唤醒所有等待的线程

t1 = threading.Thread(target=waiter)
t2 = threading.Thread(target=setter)
t1.start(); t2.start()
t1.join(); t2.join()

event.clear()               # 重置事件为未触发状态

# --- Condition：更复杂的线程协调，支持等待特定条件 ---
condition = threading.Condition()
items = []

def producer():
    for i in range(5):
        with condition:
            items.append(i)
            print(f"生产: {i}")
            condition.notify()      # 通知一个等待的消费者
        time.sleep(0.5)

def consumer():
    for _ in range(5):
        with condition:
            while not items:        # 用 while 防止虚假唤醒
                condition.wait()    # 释放锁并等待通知
            item = items.pop(0)
            print(f"消费: {item}")

t1 = threading.Thread(target=producer)
t2 = threading.Thread(target=consumer)
t1.start(); t2.start()
t1.join(); t2.join()
```

### 线程局部变量（threading.local）

```python
import threading

# threading.local 为每个线程提供独立的变量副本
local_data = threading.local()

def worker(value):
    # 每个线程的 local_data.value 互不干扰
    local_data.value = value
    import time; time.sleep(0.1)
    print(f"线程 {threading.current_thread().name}: {local_data.value}")

threads = [
    threading.Thread(target=worker, args=(i,), name=f"T-{i}")
    for i in range(5)
]
for t in threads: t.start()
for t in threads: t.join()

# 常见用途：数据库连接、请求上下文（如 Flask 的 g 对象）
```

### 守护线程

```python
import threading
import time

# 守护线程：主线程结束时，守护线程自动终止
# 非守护线程：主线程会等待所有非守护线程结束后才退出

def background_task():
    while True:
        print("后台任务运行中...")
        time.sleep(1)

# 设置为守护线程（必须在 start() 之前设置）
t = threading.Thread(target=background_task)
t.daemon = True     # 或在构造时传入 daemon=True
t.start()

time.sleep(3)
print("主线程结束，守护线程自动终止")
# 主线程结束后，守护线程不会阻止程序退出
```

## 多进程

### multiprocessing.Process

```python
import multiprocessing
import os
import time

def worker(name):
    # 每个进程有独立的内存空间和 PID
    print(f"进程 {name}, PID: {os.getpid()}")
    time.sleep(1)

if __name__ == "__main__":
    # Windows 下必须在 if __name__ == "__main__" 中创建进程
    print(f"主进程 PID: {os.getpid()}")

    p = multiprocessing.Process(
        target=worker,
        args=("子进程A",),
        name="WorkerProcess"
    )
    p.start()
    p.join()           # 等待进程结束
    print(f"退出码: {p.exitcode}")  # 0 表示正常退出

    # 批量创建进程
    processes = [
        multiprocessing.Process(target=worker, args=(f"P{i}",))
        for i in range(4)
    ]
    for p in processes: p.start()
    for p in processes: p.join()
```

### 进程间通信（Queue、Pipe）

```python
import multiprocessing

# --- Queue：多进程安全的队列 ---
def producer(queue):
    for i in range(5):
        queue.put(i)            # 放入数据
        print(f"生产: {i}")
    queue.put(None)             # 发送结束信号

def consumer(queue):
    while True:
        item = queue.get()      # 取出数据，阻塞直到有数据
        if item is None:
            break
        print(f"消费: {item}")

if __name__ == "__main__":
    q = multiprocessing.Queue()
    p1 = multiprocessing.Process(target=producer, args=(q,))
    p2 = multiprocessing.Process(target=consumer, args=(q,))
    p1.start(); p2.start()
    p1.join(); p2.join()

# --- Pipe：双向管道，适合两个进程通信 ---
def sender(conn):
    conn.send({"msg": "hello from sender"})
    conn.close()

def receiver(conn):
    data = conn.recv()          # 接收数据
    print(f"收到: {data}")
    conn.close()

if __name__ == "__main__":
    parent_conn, child_conn = multiprocessing.Pipe()  # 返回两端连接
    p1 = multiprocessing.Process(target=sender, args=(child_conn,))
    p2 = multiprocessing.Process(target=receiver, args=(parent_conn,))
    p1.start(); p2.start()
    p1.join(); p2.join()
```

### 共享内存（Value、Array）

```python
import multiprocessing

# 进程间默认不共享内存，需要使用特殊对象

def increment(shared_val, lock):
    for _ in range(1000):
        with lock:
            shared_val.value += 1   # 通过 .value 访问

if __name__ == "__main__":
    # Value：共享单个值
    # 'd' 表示 double 类型，'i' 表示 int 类型
    shared_counter = multiprocessing.Value('i', 0)
    lock = multiprocessing.Lock()

    processes = [
        multiprocessing.Process(target=increment, args=(shared_counter, lock))
        for _ in range(4)
    ]
    for p in processes: p.start()
    for p in processes: p.join()
    print(f"共享计数器: {shared_counter.value}")  # 应为 4000

    # Array：共享数组
    shared_arr = multiprocessing.Array('i', [0, 1, 2, 3, 4])
    print(list(shared_arr))     # 通过迭代访问元素
```

### 进程池（Pool）

```python
import multiprocessing
import time

def square(x):
    time.sleep(0.1)
    return x * x

if __name__ == "__main__":
    # Pool 自动管理进程的创建和复用
    with multiprocessing.Pool(processes=4) as pool:

        # map：阻塞，等待所有结果，保持顺序
        results = pool.map(square, range(10))
        print(f"map 结果: {results}")

        # map_async：非阻塞，返回 AsyncResult 对象
        async_result = pool.map_async(square, range(10))
        results = async_result.get(timeout=5)   # 设置超时
        print(f"map_async 结果: {results}")

        # starmap：支持多参数函数
        def add(a, b): return a + b
        results = pool.starmap(add, [(1, 2), (3, 4), (5, 6)])
        print(f"starmap 结果: {results}")

        # imap：惰性求值，适合大数据集
        for result in pool.imap(square, range(10)):
            print(result, end=" ")
```

## concurrent.futures

### ThreadPoolExecutor

```python
from concurrent.futures import ThreadPoolExecutor
import time

def fetch_data(url):
    time.sleep(0.5)             # 模拟 IO 操作
    return f"数据来自 {url}"

urls = [f"http://example.com/{i}" for i in range(8)]

# 使用上下文管理器，自动关闭线程池
with ThreadPoolExecutor(max_workers=4) as executor:

    # submit：提交单个任务，返回 Future 对象
    future = executor.submit(fetch_data, "http://example.com/test")
    print(future.result())      # 阻塞直到结果就绪

    # map：批量提交，返回结果迭代器（保持顺序）
    results = list(executor.map(fetch_data, urls))
    print(results)
```

### ProcessPoolExecutor

```python
from concurrent.futures import ProcessPoolExecutor

def cpu_intensive(n):
    # CPU 密集型任务，使用进程池绕过 GIL
    return sum(i * i for i in range(n))

if __name__ == "__main__":
    data = [1_000_000] * 8

    with ProcessPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(cpu_intensive, data))
        print(f"结果数量: {len(results)}")
```

### Future 对象

```python
from concurrent.futures import ThreadPoolExecutor
import time

def task(n):
    time.sleep(n)
    if n == 3:
        raise ValueError("模拟异常")
    return n * 2

with ThreadPoolExecutor(max_workers=3) as executor:
    futures = [executor.submit(task, i) for i in range(5)]

    for future in futures:
        # done()：检查是否完成（非阻塞）
        print(f"完成状态: {future.done()}")

        try:
            # result(timeout)：获取结果，超时抛出 TimeoutError
            result = future.result(timeout=5)
            print(f"结果: {result}")
        except ValueError:
            # exception()：获取任务抛出的异常
            print(f"捕获异常: {future.exception()}")
```

### as_completed 与 wait

```python
from concurrent.futures import ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED
import time
import random

def random_task(i):
    delay = random.uniform(0.1, 1.0)
    time.sleep(delay)
    return i, delay

with ThreadPoolExecutor(max_workers=5) as executor:
    futures = {executor.submit(random_task, i): i for i in range(10)}

    # as_completed：哪个先完成就先处理哪个（不保证顺序）
    for future in as_completed(futures):
        task_id = futures[future]
        result = future.result()
        print(f"任务 {task_id} 完成: {result}")

with ThreadPoolExecutor(max_workers=5) as executor:
    futures = [executor.submit(random_task, i) for i in range(10)]

    # wait：等待指定条件，返回 (done, not_done) 两个集合
    done, not_done = wait(
        futures,
        timeout=0.5,
        return_when=FIRST_COMPLETED     # 也可用 ALL_COMPLETED
    )
    print(f"已完成: {len(done)}, 未完成: {len(not_done)}")
```

## asyncio

### 事件循环

```python
import asyncio

# 事件循环是 asyncio 的核心，负责调度和执行协程
# Python 3.10+ 推荐使用 asyncio.run()，它会自动创建和关闭事件循环

async def main():
    print("在事件循环中运行")

# 推荐方式：asyncio.run() 创建新的事件循环并运行
asyncio.run(main())

# 底层方式（了解原理）
loop = asyncio.new_event_loop()
loop.run_until_complete(main())
loop.close()

# 在已有事件循环中获取当前循环
async def get_loop():
    loop = asyncio.get_event_loop()     # 获取当前运行的事件循环
    print(f"事件循环: {loop}")

asyncio.run(get_loop())
```

### async/await 语法

```python
import asyncio

# async def 定义协程函数，调用后返回协程对象（不会立即执行）
async def greet(name: str) -> str:
    await asyncio.sleep(1)      # await 挂起当前协程，让出控制权
    return f"Hello, {name}"

# await 只能在 async 函数内使用
async def main():
    # 直接 await 协程
    result = await greet("World")
    print(result)

    # 协程对象在被 await 或提交给事件循环前不会执行
    coro = greet("Python")      # 仅创建协程对象
    result = await coro         # 此时才执行
    print(result)

asyncio.run(main())
```

### 协程的创建与运行

```python
import asyncio

async def fetch(url: str) -> str:
    print(f"开始请求: {url}")
    await asyncio.sleep(1)          # 模拟网络 IO
    return f"响应: {url}"

async def main():
    # 顺序执行（总耗时 = 各任务之和）
    r1 = await fetch("url1")
    r2 = await fetch("url2")
    print(r1, r2)

    # 并发执行（总耗时 = 最长任务时间）
    results = await asyncio.gather(
        fetch("url3"),
        fetch("url4"),
    )
    print(results)

asyncio.run(main())
```

### asyncio.gather 与 asyncio.create_task

```python
import asyncio

async def task(name: str, delay: float):
    await asyncio.sleep(delay)
    print(f"任务 {name} 完成")
    return name

async def main():
    # gather：并发运行多个协程，等待全部完成，按输入顺序返回结果
    results = await asyncio.gather(
        task("A", 1.0),
        task("B", 0.5),
        task("C", 1.5),
        return_exceptions=True      # True 时异常作为结果返回，不抛出
    )
    print(f"gather 结果: {results}")  # ['A', 'B', 'C']（按输入顺序）

    # create_task：立即将协程包装为 Task 并调度执行
    # 与 gather 的区别：Task 创建后立即开始调度，不需要等待 await
    t1 = asyncio.create_task(task("D", 1.0), name="task-D")
    t2 = asyncio.create_task(task("E", 0.5), name="task-E")

    # 此时 t1、t2 已在后台运行
    print(f"任务名: {t1.get_name()}")

    r1 = await t1   # 等待 t1 完成
    r2 = await t2   # 等待 t2 完成
    print(f"Task 结果: {r1}, {r2}")

    # cancel：取消任务
    t3 = asyncio.create_task(task("F", 10.0))
    await asyncio.sleep(0.1)
    t3.cancel()                     # 发送取消请求
    try:
        await t3
    except asyncio.CancelledError:
        print("任务 F 已取消")

asyncio.run(main())
```

### asyncio.Queue

```python
import asyncio

async def producer(queue: asyncio.Queue):
    for i in range(5):
        await queue.put(i)          # 队列满时自动等待
        print(f"生产: {i}")
        await asyncio.sleep(0.2)
    await queue.put(None)           # 发送结束信号

async def consumer(queue: asyncio.Queue):
    while True:
        item = await queue.get()    # 队列空时自动等待
        if item is None:
            break
        print(f"消费: {item}")
        queue.task_done()           # 标记任务完成

async def main():
    # maxsize=0 表示无限容量
    queue = asyncio.Queue(maxsize=3)

    # 并发运行生产者和消费者
    await asyncio.gather(
        producer(queue),
        consumer(queue),
    )

asyncio.run(main())
```

### 异步上下文管理器与迭代器

```python
import asyncio

# --- 异步上下文管理器：实现 __aenter__ 和 __aexit__ ---
class AsyncResource:
    async def __aenter__(self):
        print("异步获取资源")
        await asyncio.sleep(0.1)    # 模拟异步初始化
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        print("异步释放资源")
        await asyncio.sleep(0.1)    # 模拟异步清理
        return False                # False 表示不抑制异常

    async def do_work(self):
        return "工作完成"

async def main():
    # 使用 async with
    async with AsyncResource() as res:
        result = await res.do_work()
        print(result)

    # --- 异步迭代器：实现 __aiter__ 和 __anext__ ---
    class AsyncCounter:
        def __init__(self, stop: int):
            self.current = 0
            self.stop = stop

        def __aiter__(self):
            return self

        async def __anext__(self):
            if self.current >= self.stop:
                raise StopAsyncIteration    # 迭代结束
            await asyncio.sleep(0.1)        # 模拟异步操作
            value = self.current
            self.current += 1
            return value

    # 使用 async for
    async for num in AsyncCounter(5):
        print(f"异步迭代: {num}")

    # --- 异步生成器（更简洁的写法）---
    async def async_range(n: int):
        for i in range(n):
            await asyncio.sleep(0.05)
            yield i                         # async def + yield = 异步生成器

    async for val in async_range(5):
        print(f"异步生成器: {val}")

asyncio.run(main())
```

## 并发模型对比

### 多线程 vs 多进程 vs 协程的适用场景

```python
# 多线程适用场景
# - IO 密集型：文件读写、网络请求、数据库操作
# - 需要共享内存状态
# - 任务数量适中（数十到数百）

# 多进程适用场景
# - CPU 密集型：数值计算、图像处理、数据压缩
# - 需要绕过 GIL 实现真正并行
# - 任务相对独立，不需要频繁通信

# 协程（asyncio）适用场景
# - 高并发 IO：同时处理数千个网络连接
# - 单线程，无锁，开销极小
# - 需要精细控制并发流程

def choose_concurrency_model(task_type: str, concurrency: int) -> str:
    if task_type == "cpu_intensive":
        return "multiprocessing.Pool 或 ProcessPoolExecutor"
    elif task_type == "io_intensive":
        if concurrency > 1000:
            return "asyncio（协程）"
        else:
            return "ThreadPoolExecutor 或 asyncio"
    else:
        return "根据具体场景测试后决定"

print(choose_concurrency_model("cpu_intensive", 8))
print(choose_concurrency_model("io_intensive", 5000))
```

### 性能对比表格

| 维度 | 多线程 | 多进程 | asyncio 协程 |
|---|---|---|---|
| 并行能力 | 受 GIL 限制，伪并行 | 真正并行 | 单线程并发，非并行 |
| 内存开销 | 低（共享内存） | 高（独立内存空间） | 极低 |
| 创建开销 | 中 | 高 | 极低 |
| 通信方式 | 共享变量（需加锁） | Queue/Pipe/共享内存 | 直接传递（无需加锁） |
| 适用任务 | IO 密集型 | CPU 密集型 | 高并发 IO |
| 调试难度 | 中（竞态条件） | 中 | 低（单线程） |
| 最大并发数 | 数百 | 受 CPU 核心数限制 | 数万 |
| Python 版本 | 全版本 | 全版本 | 3.4+（3.7+ 推荐） |
